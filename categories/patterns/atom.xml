<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">

  <title><![CDATA[Category: Patterns | Ivan Voroshilin's Blog.]]></title>
  <link href="http://vibneiro.github.io/categories/patterns/atom.xml" rel="self"/>
  <link href="http://vibneiro.github.io/"/>
  <updated>2015-03-14T00:11:02+03:00</updated>
  <id>http://vibneiro.github.io/</id>
  <author>
    <name><![CDATA[Ivan Voroshilin]]></name>
    <email><![CDATA[mail@ivoroshilin.com]]></email>
  </author>
  <generator uri="http://octopress.org/">Octopress</generator>

  
  <entry>
    <title type="html"><![CDATA[The Flip Side of Rule Engines on Example of Drools and Some Valuable Tips]]></title>
    <link href="http://vibneiro.github.io/2014/10/12/the-flip-side-of-rule-engines-and-some-tips-on-when-not-use-ones/"/>
    <updated>2014-10-12T02:21:19+04:00</updated>
    <id>http://vibneiro.github.io/2014/10/12/the-flip-side-of-rule-engines-and-some-tips-on-when-not-use-ones</id>
    <content type="html"><![CDATA[<p>Hey there! This post is devoted to business rule engines and solely written based on my research and experience with Open-Source <a href="http://www.drools.org">Drools</a> engine that has been used in Production for more than 1 year in mission-critical systems. Undoubtedly, this engine is great for many features. But, I want to explain you what you should think of before using it or when not to use it all. Because it is very important step, once you adapted it in your project, it will be quite costly to remove one later.  I am going to be quick on the theory here, pointing you at issues and important things. If you don&rsquo;t understand something, feel free to ask me but first please refer to the official documentation. I&rsquo;ll try to underline the major issue I&rsquo;ve come across, surely there are many more.</p>

<p>A rule engine is not a magical box that does something new . It is intended to be a tool that provides a higher level of abstraction so you can focus less on reinventing the wheel.</p>

<p>On one of our projects we had a critical system with thousands of small &ldquo;if then else if&hellip;&rdquo; requirements from business that were initially developed as a sequence of checks on java. Gradually, we realized how this clutter turned into a very difficult mess to maintain. Hence we made a decision to apply some open source rule engine framework. After some high-level research Drools framework became our choice, primarily because it is free and open source. No one had experience on the team with commercial rule engines, however.</p>

<p>Ok, let&rsquo;s get to it!</p>

<p><img src="https://ivoroshilin.files.wordpress.com/2014/10/rule-engine-inkscape.png" alt="rule-engine-inkscape" /></p>

<h3><strong>The order of rules execution</strong></h3>

<p>Rule-engines are a good fit if you don&rsquo;t know the order in which rules are executed, e.g. the order doesn&#8217;t matter. Frequently, in order to understand this, you have to decompose your logic in such a way that you have separated simple flat rules not depending on each other.  In a nutshell, Rule engine is like a black box that have inputs and outputs. This is a necessary but not sufficient condition. Besides, when rules change objects data, that are used in other rules, it greatly increases complexity. Hence you always should be vigilant on the order of execution of different rules.</p>

<p>Otherwise, if the order is determinate, frequently it would be better-off using BPM-engines or something else.</p>

<h3><strong>Clear domain model with stable public API</strong></h3>

<p>Rule-engines are not a fit if your system doesn&rsquo;t have a <strong>clear domain model.</strong> Most non-mature systems don&rsquo;t have one. It&rsquo;s evolution, sometimes requiring to understand with time. Even clear requirements not always so clear in practice. Why am I saying this? Well,  because a good rule of thumb it that:</p>

<blockquote>&#8220;Rules should be working with only the Domain Model of your system with clear public API around it. If they don&#8217;t, your rules ultimately will turn into unmanageable mess. Period!&#8221;</blockquote>


<p>It seems to be natural, but many developers don&rsquo;t realize it. Imagine, what happens if you rules start using internal code or other API that is not a part of domain logic.  With such a disgusting approach, the rules will be aware of more then they need to, like internal implementations/behaviors  and other dependencies =<strong> tightly coupled code</strong>. It&rsquo;s just like in OOP or SOA principles but in terms of rules. Tightly coupled code/rules is very difficult to change. Needless to say, if this code is not encapsulated. Eventually, you will have to modify your code triggering broken rules very often, when business requirements change.</p>

<h3>Indepotence or exactly once</h3>

<p>All rules must run multiple times without errors yielding the same result. It&rsquo;s like writing a bash-script and also considered a good rule of thumb. Otherwise you may encounter lots of misbehaving weird things.</p>

<h3><strong>Forget Debugging tools, %100 coverage with tests is needed</strong></h3>

<p>Rule-engines are difficult to refactor, you would have to analyze them and/or keep the whole graph in mind if there are dependencies. In terms of Drools it is impossible to trace rules line-by-line. Drools-rules are declarative and don&rsquo;t have a mechanism to debug them. Therefore, you should ideally cover 100% of your rules with JUnit-tests. I can give you a hint to use Spock-framework on top of Groovy as a Behavioural-Driven Development stuff, where each rule is simply covered with a test. Tests usually come directly from business specs. Each rule should contain only 1 simple thing as minimal as possible.</p>

<h3>Cost of change</h3>

<p>No one has any idea if there are conflicting rules when a new one is added or existing one is changed.  A complete regression test must be done when rules are changed. The Memory is sometimes a big issue too. And you can do very little to reduce the consumption - you use 3-rd party framework.</p>

<h3><strong>Some rules need to be triggered twice or more</strong></h3>

<p>Somebody of you can claim that Drools has a feature of rule-invalidation of Facts, but this again turns into unobvious, tangled mess getting in the way to quickly understand why rules yielded such a result. It&rsquo;s up to you, but you never know whether you will have to flush/invalidate rules in the long run.</p>

<h3>Rete Algorithm vs your Domain API and traversals</h3>

<p>The majority of modern rule engines including Drools work upon so called <a href="http://en.wikipedia.org/wiki/Rete_algorithm">Rete Algorithm</a>. Make sure that your codebase, which your rules call, can be built by a Rete algorithm in the working memory. If it&rsquo;s not, tree traversal of your codebase within rules on such a model will be a big performance issue!  Bear in mind, if you have internal traversal logic inside your public domain API (e.g. some of your methods do traversal for you) happening on LHS (Left hand side operation) clause , it is very bad for Drools&#8217; performance as latter has its own <strong>flexible querying model directly</strong> related to Rete-tree. Drools itself should be able to build the tree for you based on your API and efficiently traverse it.</p>

<h3>Performance and external systems</h3>

<p>What if you are required to save a set of operations to a database and each operation must be checked via rule engine. And the next operation depends on the result of the previous one. This imposes performance problems. All operation could be validated and afterwards saved in 1 go. Think it through in advance. I know there are Rule Groups in Drools, but my point - it&rsquo;s very complicated.</p>

<h3>Rules Centralization</h3>

<p>The same business rules may apply across different services, leading to redundancy and governance challenges. It imposes a burden upon us to keep the content of those rules in synch over time.  Thus, it is usually recommended to move them to a new dedicated rule service. The downside is that such a service becomes an additional architectural dependency for other services. The responsibility to build and maintain centralized rule service can introduce various headaches. When we need to change currently active rules or introduce new ones we need to ensure that these changes to the existing rule architecture do not have negative or unforeseen impacts, the worst of which can be a cascading effect that causes exceptions that compound across multiple rules (and multiple services), which should be thoroughly tested.</p>

<p>And please! Make sure that your services know nothing about rules but only talk to the service via a clear contract.</p>

<h3>Conclusion</h3>

<p>This write-up only scratched the surface of issues coming from my experience. To be honest with you, I would say &ldquo;No&rdquo; to applying rule engine frameworks in my future projects to a large number of systems. Initially it seems to be pretty easy, but eventually you realize, that third-party rule engines are more of a pain that benefit, especially when it comes to interdependencies and a domain model. Project maintenance becomes very costly due to this.  It&rsquo;s not as simple as coding a bunch of <code>if </code>statements. The rules engine gives you the tools to handle rule relationships, but you still have to be able to imagine all of that in your mind.</p>

<p>The most important thing, it is quite difficult to use a rule engine effectively and correctly. A team should have solid knowledge of how to use it and understanding with implementation of business model should be mature. Rule engines work well if there is a really flat independent logic among rules/facts. Each framework restricts you to use their specific model. It&rsquo;s not worth it!</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Command and Query Responsibility Segregation and Event Sourcing: What You Should Think About in Advance]]></title>
    <link href="http://vibneiro.github.io/2014/08/14/command-and-query-responsibility-segregation-and-event-sourcing-what-you-should-think-about-in-advance/"/>
    <updated>2014-08-14T22:35:09+04:00</updated>
    <id>http://vibneiro.github.io/2014/08/14/command-and-query-responsibility-segregation-and-event-sourcing-what-you-should-think-about-in-advance</id>
    <content type="html"><![CDATA[<p>After quite a while this article has finally come out! I started to write it about 6 months ago but because of many factors in my life I procrastinated. This text is mainly devoted to those who want to see major pros and cons of this model. You won&rsquo;t find here the nuts and bolts however. For this, move to the reference section where listed really cool URLs for the start. Further the pattern will be named as CQRS/ES for simplicity.</p>

<h1>2 main goals of the pattern</h1>

<p>Some developers consider that this pattern is solely used for scalability reasons forgetting that one greatly reduces data model complexity.</p>

<h1>Events and snapshots - no removal but always append</h1>

<p>it is well known that append-only architectures distribute more easily than updating architectures because there are far fewer locks to deal with. Append-only operations require no locks on a database-side which increases both performance and scalability.</p>

<h1>CQRS + Event Model</h1>

<p>One of the largest issues when using Event Sourcing is that you cannot ask the system a query such as “Give me all users whose first names are ‘Greg’”. This is due to not having a representation of current state.</p>

<p>With Event Sourcing <strong>the event model</strong> is also the persistence model <strong>on the Write side</strong>. This drastically lowers costs of development as <strong>no conversion between the models is needed</strong>.</p>

<h1>Synchronizing write and read sides</h1>

<p><strong>The choice of integration model</strong> though is very important as translation and synchronization between models <strong>can be become a very expensive undertaking</strong>. The model that is best suited is the introduction of events<strong>, events are a well known integration pattern</strong> and offer the best mechanism for model synchronization.</p>

<p>CQRS suggests using 2 different databases: one optimized for reading (denormalized) and another for writing. It is a very tempting way to designing systems. The only big problem with this solution is that you need somehow to sync up your 2 data storages.</p>

<p>You must definitely have a mechanism to recreate a read-side from the write-side. You might need to do this if the read side data store got out of synchronization for some reason, or because you needed to modify the structure of the read-side data store to support a new query. Remember that the CQRS pattern does not mandate that you use different stores on the read side and write side. You could decide to use a single relational store with a schema in third normal form and a set of denormalized views over that schema. However, replaying events is a very convenient mechanism for resynchronizing the read-side data store with the write-side data store.</p>

<h1>Storage technology replaceability</h1>

<p>CQRS/ES makes it easy to change your technologies. For example, you could start with a file-based event store for prototyping and development, and later switch to a Windows Azure table-based store for production.</p>

<h1>Relational vs NoSQL solutions and the cost of maintenance - think twice</h1>

<p>Let&rsquo;s mention major issues in case of using a relational database as a technology for this model.</p>

<h2><strong>Pitfall 1: Limit in number of columns per table</strong></h2>

<p>Read-side is always kept in a denormalized form for performance reasons and usually requires a huge number of columns. If you have e.g. Oracle 11g database and hit a maximum number of 1000 columns per table you won&rsquo;t be able to add more columns.</p>

<h2><strong>Pitfall 2: Adding new columns</strong></h2>

<p>In many cases in order to add new data to the domain model you must release a bunch of applications/services along with changing a schema of a database on the read-side, especially when a column is not empty but already has some data. Here, I prefer a NoSQL databases such as MongoDB with relaxed schema that doesn&rsquo;t require to change one (data are kept in JSON documents) thus<strong> you won&rsquo;t have to change the schema of a database at all! </strong>CQRS/ES patterns with NoSQL db might have a simple Event Storage for the write-side with a blob-column in which data are represented as history and read-side as a schema-less NoSQL db.</p>

<h1>Eventual consistency - think twice</h1>

<p>While not required, it is common to use messaging between the write and read sides. This means that the system will be in an inconsistent state from time to time. This pattern is not a fit for e.g. financial transactions where a strong consistency is required between 2 sides. However, in majority of cases strong consistency can be used only on the write-side whereas the read-side is used for querying and reporting.</p>

<h1>Simple vs costly</h1>

<p>Events are simple objects that describe what has happened in the system. By simply saving events, you are avoiding the complications associated with saving complex domain objects to a relational store; namely, the object-relational impedance mismatch.</p>

<p>The event based model may be slightly more costly due to the need of definition of events but this cost is relatively low and it also offers a smaller Impedance Mismatch to bridge which helps to make up for the cost. The event based model also offers all of the benefits discussed in “Events” that also help to reduce the overall initial cost of the creation of the events. That said the CQRS and Event Sourcing model is actually less expensive in most cases. That said the CQRS and Event Sourcing model is actually<strong> less expensive in most cases.</strong></p>

<h1>Flexibility</h1>

<p>As long as you have a stream of events, you can project it to any form, even a conventional SQL database. For instance, my favorite approach is to project event streams into JSON documents stored in a cloud storage.</p>

<p>Also a great benefit of the separate read layer is that it will not suffer from an <strong>impedance mismatch -</strong> It is connected directly to the data model, this can make queries much easier to optimize. On the write side you no longer need to worry about how your locks impact queries, and on the read side your database can be read-only.</p>

<h1>Performance</h1>

<p>Write-side:</p>

<p>Because events are immutable, you can use an append-only operation when you save them. Events are also simple, standalone objects. Both these factors can lead to better performance and scalability for the system than approaches that use complex relational storage models. On top of this events reduce network volumes unlike full snapshots in case of a pure CQRS model.</p>

<p>Read-Side:</p>

<p>A normalized database schema can fail to provide adequate response times because of the excessive table JOIN operations. Despite advances in relational database technology, a JOIN operation is still very expensive compared to a single-table read.</p>

<h1>Scalability</h1>

<p>Command: In most systems, especially web systems, the Command side generally processes a very small number of transactions as a percentage of the whole. Scalability therefore is not always important.</p>

<p>Query: In most systems, especially web systems, the Query side generally processes a very large number of transactions as a percentage of the whole (often times 2 or more orders of magnitude). Scalability is most often needed for the query side.</p>

<h1>Distributed development teams</h1>

<p>The architecture can be viewed as three distinct decoupled areas. The first is the client; it consumes DTOs and produces Commands. The second is the domain; it consumes commands and produces events. The third is the Read Model; it consumes events and produces DTOs. The decoupled nature of these three areas can be extremely valuable in terms of team characteristics. Instead of working in vertical slices the team can be working on three concurrent vertical slices, the client, the domain, and the read model. This allows for a much better scaling of the number of developers working on a project as since they are isolated from each other they cause less conflict when making changes. It would be reasonable to nearly triple a team size without introducing a larger amount of conflict due to not needing to introduce more communication for the developers.</p>

<h1><strong>Conclusion</strong></h1>

<p>There is no one right way to implement CQRS. It is not recommended to use this model for critical projects of course if you have no experience with one. Note that It is not possible to create an optimal solution for searching, reporting, and processing transactions utilizing a single model. </p>

<h1>References</h1>

<p><a href="http://www.kenneth-truyers.net/2013/12/05/introduction-to-domain-driven-design-cqrs-and-event-sourcing/%C2%A0">Introduction to Domain Driven Design, CQRS and Event Sourcing</a></p>

<p><a href="http://www.bennadel.com/blog/2438-command-query-responsibility-segregation-cqrs-makes-domain-models-practical.htm">Command-Query-Responsibility-Segregation (CQRS) Makes Domain Models Practical</a></p>

<p><a href="http://codebetter.com/gregyoung/2010/02/13/cqrs-and-event-sourcing/">CQRS and Event Sourcing</a></p>

<p><a href="http://cqrs.wordpress.com">CQRS - Did you mean CARS?</a></p>

<p><a href="http://msdn.microsoft.com/en-us/library/jj591577.aspx">A CQRS and ES Deep Dive</a></p>

<p><a href="http://www.udidahan.com/wp-content/uploads/Clarified_CQRS.pdf">Clarified CQRS</a></p>

<p> </p>

<p> </p>

<p> </p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Service Discovery in Distributed Systems]]></title>
    <link href="http://vibneiro.github.io/2014/02/17/service-discovery-in-distributed-systems/"/>
    <updated>2014-02-17T15:28:01+04:00</updated>
    <id>http://vibneiro.github.io/2014/02/17/service-discovery-in-distributed-systems</id>
    <content type="html"><![CDATA[<p>After quite a while I&rsquo;ve decided to continue writing my blog. There are always lots of ideas to share with you but as usual a lack of time.</p>

<h1><strong>Introduction</strong></h1>

<p>Service Discovery is an architectural pattern and a key component of most distributed systems and service oriented architectures.</p>

<p> In simple terms it is a central gateway for all client applications that want to use other services. A client application doesn&rsquo;t have to know where a particular service is located (usually IP:port), moreover a service can be moved/redeployed to arbitrary boxes for any reasons. There&rsquo;s no need to change connection details, update configs or whatever - Service Discovery will take care of that. You as a client-app just need to ask it to get access to other services. The pattern yields a good benefit especially when there are hundreds of client applications and/or dozens of services.</p>

<p>Most of the work in this article was made relying on trials and errors as well some research. I urge you to read also a great survey from Jason Wilder&rsquo;s blog on <a href="http://jasonwilder.com/blog/2014/02/04/service-discovery-in-the-cloud/">Open Source Service Discovery</a> frameworks.</p>

<p>In this particular section I won&rsquo;t mention CAP-properties as they can vary depending on implementation. But CAP is important. If you want to know more about it I recommend moving to <a href="http://ivoroshilin.com/2012/12/13/brewers-cap-theorem-explained-base-versus-acid/">this article</a> devoted to the CAP-theorem.</p>

<h2>High level components</h2>

<p>Service Discovery is the process of finding a suitable Service and its location for a given task that asked a service consumer.  Let&rsquo;s break down Service Discovery into 3 main components:</p>

<p><strong>Service Requestor</strong></p>

<p>Any consuming application that want to use some service,  that is a client application or a consumer, user, etc.</p>

<p><strong>Service Provider</strong></p>

<p>In the most basic scenario there is a Service Provider that finds this service and talks to Service Requestor.</p>

<p><strong>Service Registry</strong></p>

<p>Service Registry stores information about all services. It can be either static or dynamic. This can be IP:port, access rights and so forth.</p>

<p>This model comes directly from SOA but can be adapted to your needs. Conversation among 3 above entities might differ depending on implementation. As a quick example of such segregation refer to <a href="http://en.wikipedia.org/wiki/Web_Services_Discovery">Web Service Discovery</a>.</p>

<h2>Discovery options</h2>

<h3>1. Static service registry</h3>

<p>This one is a very basic and easy option. A big drawback - it is static. Every time a service moves to another box, static repo should be updated manually. We won&rsquo;t go further here as it is quite obvious and have little benefit for flexibility.</p>

<h3>2. Announcement</h3>

<p>This pattern implies that as soon as a service is up, it registers itself in the <strong>Service Registry - </strong>sometimes such a notification is called<strong> Announcement. </strong>This simple approach greatly simplifies maintenance of service registry. It&rsquo;s worth mentioning  also that in distributed world we always care about <strong>No Single Point Of Failure</strong> principle<strong> </strong>where we have at least 2 redundant nodes for a service. Service Registry is usually implemented as a separate  independent component to be self-sufficient. Just for simplicity the registry can be a simple replicated memCached, ehCache or another storage, not necessarily cache.</p>

<h3>A few words about ZooKeeper as a solution for Service Discovery.</h3>

<p>A counterexample is ZooKeeper that does&rsquo;t follow <strong>Shared Nothing architectural </strong>pattern. Service Registry is coupled with the Service Provider. In ZooKeeper&rsquo;s world coordination-nodes in ensemble are NOT independent of each other because they share Service Registry and there&rsquo;s a write-contention due to the order of messages. The last word leads to ZooKeeper&rsquo;s poor write-scalability due to its synchronous replication among nodes. But in terms of requirements most applications involved into configuration management including Service Discovery don&rsquo;t require frequent writes but rather reads not blocking overall scalability.</p>

<p>Let&rsquo;s pick up where we left off -  announcement.</p>

<p>The process of announcement is depicted below:</p>

<h3></h3>

<h3><a href="http://ivoroshilin.files.wordpress.com/2014/02/announcement_pb.png"><img src="http://ivoroshilin.files.wordpress.com/2014/02/announcement_pb.png" alt="Announcement_pb" /></a></h3>

<h3></h3>

<h3></h3>

<h3></h3>

<h3>3. Multicasting</h3>

<p>Using IGMP-protocol, a Service provider subscribes to a particular group, all services send a multicast message to this group. The downside of this approach is that IGMP is optional for IPv4 and not every hardware supports it.</p>

<h3>Health checking</h3>

<p>Once a service is registered in Service Registry a Service Provider needs to know if it&rsquo;s alive and healthy (that is, 100% of service is available).</p>

<p><strong>Option 1 - on reconnect:</strong> As soon as a Service Requestor understands that there&rsquo;s no connection to the service it worked with, it reverts back to the Service Provider to get a new one. Each time a Service Requestor accesses Service Provider, one does health-checking (let&rsquo;s say it is almost the same as heartbeats plus probably some other stuff) to make sure that the service is alive, maintaining the Service Registry and returning response to the client containing information about a healthy service.</p>

<p><strong>Option 2 - heartbeating:</strong> Service Provider receives heartbeats within equal time-slices from registered services. Once heartbeat is lost, the Service Provider removes corresponding record from the Service Registry. This is a good option to get a more real-time information though it is more difficult to implement. This way works ZooKeeper and other distributed systems.</p>

<p><a href="http://ivoroshilin.files.wordpress.com/2014/02/service-down_pn.png"><img src="http://ivoroshilin.files.wordpress.com/2014/02/service-down_pn.png" alt="Service Down_pn" /></a></p>

<p>In both options if some service goes down clients get back to Service Provider. What strategy to use is up to the developer but option 1 is more complex but a better one in most scenarios.</p>

<h3>Load Balancing</h3>

<p>Load balancing can be easily implemented inside Service Provider as one becomes a central gateway of access for all Service Requestors to other services. For the sake of simplicity a very simple variation is round-robin. I won&rsquo;t go into details as it is a different topic and you can find on the internet.</p>

<h3>Authentication</h3>

<p>Again as with load balancing you might want to apply tokens with expiration or cryptographic keys.</p>

<h2>Cooperation of all components</h2>

<p>On a very high level the communication of components can happend as on the following picture:</p>

<p><a href="http://ivoroshilin.files.wordpress.com/2014/02/service-request_pb.png"><img src="http://ivoroshilin.files.wordpress.com/2014/02/service-request_pb.png" alt="Service Request_pb" /></a></p>

<h3>Summary</h3>

<p>We have defined what a Service Discovery is, broke it down into 3 components and described a few basic approaches.</p>
]]></content>
  </entry>
  
</feed>
