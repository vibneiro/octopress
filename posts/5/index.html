
<!DOCTYPE html>
<!--[if IEMobile 7 ]><html class="no-js iem7"><![endif]-->
<!--[if lt IE 9]><html class="no-js lte-ie8"><![endif]-->
<!--[if (gt IE 8)|(gt IEMobile 7)|!(IEMobile)|!(IE)]><!--><html class="no-js" lang="en"><!--<![endif]-->
<head>
  <meta charset="utf-8">
  <title>Ivan Voroshilin&#8217;s Blog.</title>
  <meta name="author" content="Ivan Voroshilin">

  
  <meta name="description" content="  Introduction In the previous post we briefly described what might stand beyond asymptotic analysis of algorithms and data structures when it comes &hellip;">

  
  <meta name="keywords" content="distributed, algorithm, Ivan, Voroshilin, code, google, jam, software, architecture, geek, blog, java, scala, groovy, scalability" />


  <!-- http://t.co/dKP3o1e -->
  <meta name="HandheldFriendly" content="True">
  <meta name="MobileOptimized" content="320">
  <meta name="viewport" content="width=device-width, initial-scale=1">

  
  <link rel="canonical" href="http://vibneiro.github.io/posts/5/">
  <link href="/favicon.png" rel="icon">
  <link href="/stylesheets/screen.css" media="screen, projection" rel="stylesheet" type="text/css">
  <link href="/octopress/atom.xml" rel="alternate" title="Ivan Voroshilin's Blog." type="application/atom+xml">
  <script src="/javascripts/modernizr-2.0.js"></script>
  <script src="//ajax.googleapis.com/ajax/libs/jquery/1.9.1/jquery.min.js"></script>
  <script>!window.jQuery && document.write(unescape('%3Cscript src="/javascripts/libs/jquery.min.js"%3E%3C/script%3E'))</script>
  <script src="/javascripts/octopress.js" type="text/javascript"></script>
  <!--Fonts from Google"s Web font directory at http://google.com/webfonts -->
<link href="//fonts.googleapis.com/css?family=PT+Serif:regular,italic,bold,bolditalic" rel="stylesheet" type="text/css">
<link href="//fonts.googleapis.com/css?family=PT+Sans:regular,italic,bold,bolditalic" rel="stylesheet" type="text/css">

  
<script>
  (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
  (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
  m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
  })(window,document,'script','//www.google-analytics.com/analytics.js','ga');

  ga('create', 'UA-37693662-1', 'ivoroshilin.com');
  ga('send', 'pageview');

</script>


</head>

<body   >
  <header role="banner"><hgroup>
  <h1><a href="/">Ivan Voroshilin&#8217;s Blog.</a></h1>
  
    <h2>Algorithmic contests, distributed systems and software architecture</h2>
  
</hgroup>

</header>
  <nav role="navigation"><ul class="subscription" data-subscription="rss">
  <li><a href="/octopress/atom.xml" rel="subscribe-rss" title="subscribe via RSS">RSS</a></li>
  
</ul>
  
<form action="https://www.google.com/search" method="get">
  <fieldset role="search">
    <input type="hidden" name="sitesearch" value="vibneiro.github.io">
    <input class="search" type="text" name="q" results="0" placeholder="Search"/>
  </fieldset>
</form>
  
<ul class="main-navigation">
  <li><a href="/">Main page</a></li>
  <li><a href="/blog/archives">All posts</a></li>
  <li><a href="/talks">Talks</a></li>
  <li><a href="/about">About</a></li>
</ul>

</nav>
  <div id="main">
    <div id="content">
      <div class="blog-index">
  
  
  
    <article>
      
  <header>
    
      <h1 class="entry-title"><a href="/2013/02/06/know-your-locality-of-reference-some-techniques-for-keeping-data-in-the-cpu-cache/">What You Should Know About Locality of Reference</a></h1>
    
    
      <p class="meta">
        




<time class='entry-date' datetime='2013-02-06T17:18:51+04:00'><span class='date'><span class='date-month'>Feb</span> <span class='date-day'>6</span><span class='date-suffix'>th</span>, <span class='date-year'>2013</span></span> <span class='time'>5:18 pm</span></time>
        
           | <a href="/2013/02/06/know-your-locality-of-reference-some-techniques-for-keeping-data-in-the-cpu-cache/#disqus_thread"
             data-disqus-identifier="http://vibneiro.github.io/2013/02/06/know-your-locality-of-reference-some-techniques-for-keeping-data-in-the-cpu-cache/">Comments</a>
        
      </p>
    
  </header>


  <div class="entry-content"><h2><a href="http://ivoroshilin.com/2013/02/06/know-your-locality-of-reference-some-techniques-for-keeping-data-in-the-cpu-cache/"> </a></h2>

<h2><strong>Introduction</strong></h2>

<p>In the previous post we briefly described what might stand beyond asymptotic analysis of algorithms and data structures when it comes to empirical measurements of performance. In this post I continue talking about latter considering more closely the impact of memory hierarchies of modern computer architectures. A few basic data structures are taken for comparison of locality utilization with short explanations. Further, we&rsquo;re going to touch on effective utilization of CPU-cache by showing some techniques to improve performance.  All benchmarks are done on JVM HotSpot 6. Due to different algorithms of GC and memory allocation, the techniques might not / partly work on other platforms, but the idea to improve locality should work on the majority of modern CPU-architectures. I advice that every software developer should read <a href="http://t.co/nTxQAgzB">this excellent article</a> (especially &ldquo;sections 3 and 6) to better understand CPU-caches and memory and then come back to this post.</p>

<h2><strong>Memory hierarchies and difference in speed</strong></h2>

<p>There&rsquo;s a huge gap between the speed of CPUs and the latency of DRAM-memory. CPU&rsquo;s cache-memory is roughly 100 times faster than main memory which in turn 10k times faster than secondary storage. Thus, the processor will have to wait more than 100 cycles every time the memory is needed to deliver data. This problem is solved by sticking smaller, faster memory chips in between the processor and the main memory. These chips are called CPU-caches. Techniques in which cache is heavily utilized during the execution of a program can dramatically impact the performance of algorithms. Caches improve performance when memory accesses exhibit locality.</p>

<h2>**Locality of reference</h2>

<p>**</p>

<p>Hierarchical memory is a hardware optimization that takes the benefits of spatial and temporal locality and can be used on several levels of the memory hierarchy. <a href="http://en.wikipedia.org/wiki/Locality_of_reference">Locality</a> describes the same value or related <a href="http://www.wikipedia.org/wiki/Computer_storage">storage</a> locations being frequently accessed from memory. There are two basic types of locality:</p>

<ul>
<li><p>Temporal locality refers to the reuse of specific data and/or resources within relatively small time durations. Access latency can be avoided by reusing the data fetched previously. With poor temporal locality, data that could have been reused is lost from a working area and must be fetched again. Loops over the same small amount of data result in excellent temporal locality.</p></li>
<li><p>Spatial locality refers to the use of data elements within relatively close storage locations. The concept that likelihood of referencing a resource is higher if a resource near it was just referenced. With this type of locality, access latency can be amortized by retrieving multiple data items in a single fetch cycle and using good spatial locality to ensure that all the data fetched is useful. If the data is not useful, because of poor spatial locality, then more fetch cycles will be required to process the same data. A simple array is a good candidate for this type of locality.</p></li>
</ul>


<p>When high speed is needed it is very important to understand what data structures exhibit better locality of CPU-caches and how to improve one. Good locality is a good speed of data access affecting both throughput and latency. For CPU-caches the most common replacement policy is to fetch blocks from slower memory (DRAM) into fast memory (SRAM) 1 cache-line at a time when block of memory is touched by a program which doesn&rsquo;t reside in the cache, a kind of LRU that might differ not significantly from pure LRU-policy. Note that how the program-level locality is mapped onto memory-level depends on the compiler&rsquo;s layout of objects and on the allocator&rsquo;s placement of those objects in memory in the operating system. Most compilers layout data in consecutive areas of memory. Normally, compilers preserve the order of variables which helps to achieve a good locality on a program-level. The conventional wisdom is that programs spend 90% of their time executing 10% of the code.  By placing the most common instructions and data in the fast-but-small storage, while leaving the rest in the slow-but-large storage, we can lower the average memory-access time of a program significantly.  Next, we consider locality effects on classic algorithms and data structures with techniques.</p>

<h2><strong>Arrays, linked lists and locality</strong></h2>

<p>Now let&rsquo;s consider a simple analysis by example exhibiting poor locality of reference.</p>

<p>I run the benchmark on an Intel Core 2 Duo CPU 3GHz using:</p>

<ul>
<li><p>JDK: 1.6.0_27</p></li>
<li><p>JVM-params: -Xms512m -Xmx512m -XX:CompileThreshold=1 -XX:newRatio=128                                      -XX:+UseConcMarkSweepGC -XX:+CMSIncrementalMode</p></li>
</ul>


<p>The benchmark was taken 100 times in a row to calculate average values. I performed a simple test by appending a single element in a loop N times to the end of a LinkedList and then did the same for an ArrayList. Mind you, the complexity of the operation on both collections is equal O(1).</p>

<p><img src="http://ivoroshilin.files.wordpress.com/2013/02/graph.png" alt="graph" /></p>

<p>As can be seen on the graph ArrayList is a winner.  In this benchmark I tried to do the same test with preallocated array for the first test and by default for the second one. Of course, for non-preallocated ArrayList total time grew higher, but nevertheless one has beaten LinkedList with the above figures.</p>

<p>The thing is that <em>a dynamic array</em> allocates all elements contiguously in memory usually in a single continuous block of memory whereas a linked list contains its elements fragmentally. That is, nodes of a linked list can be scattered in arbitrary areas of memory. As have been said in the beginning caches of modern processors don&rsquo;t like random access to memory. Therefore, sequential access in arrays is faster than on linked lists on many machines, because they have a very good <a href="http://en.wikipedia.org/wiki/Locality_of_reference">locality of reference</a> and thus make good use of data caching.</p>

<p>Recall that LinkedList will have to do additionally a memory allocation on each insertion of a new element. But, wait, access to elements in the memory takes longer than from CPU-cache. You can check this using a bit different test by iterating these two data structures from left to right with the exact parameters as above and see. Again, the iteration time is slower in LinkedList due to locality of reference. The ArrayList<code> </code>elements are closer together, so there are fewer <a href="https://secure.wikimedia.org/wikipedia/en/wiki/CPU_cache">cache misses</a> indeed. Even when the linked list does not include any cache-misses at all, the traversal will be slower. This is because visiting each item incurs the cost of two dereference operations instead of the single one that the array list needs, taking double the amount of time. Cache-misses are major selling point of the array backed up structures. Linked structures are potentially a cache miss on each node making the O(n) iteration actually significantly slower. The append (insert at end) time for ArrayList is <em>amortized</em> O(1), meaning that it&rsquo;s O(1) on average when doing a long series of appends. Any one of them may be O(n), but that one doubles the capacity so the next <em>n</em> of them can use pre-allocated space.  Of course, periodically the ArrayList&rsquo;s backing array may need to be resized (which won&rsquo;t be the case if it was chosen with a large enough initial capacity), but since the array grows exponentially the amortized cost will be low, and is bounded by O(lg n) complexity.</p>

<p>Further optimizations:</p>

<ul>
<li><p><a href="http://en.wikipedia.org/wiki/Unrolled_linked_list">Unrolled LinkedList</a> - it can dramatically increase <a href="http://en.wikipedia.org/wiki/CPU_cache">cache</a> performance, while decreasing the memory.</p></li>
<li><p><a href="http://java.dzone.com/articles/false-sharing">False sharing</a> - cache-optimization for arrays in SMP CPU-architecture - another technique used in multithreading.</p></li>
</ul>


<p><strong>The bottom line: </strong>Arrays are superior at exploiting CPU-cache in a sequential access unlike linked data structures. Thanks to spatial locality! The problem of linked lists is when the node is accessed, the whole cache line is fetched from main memory, yet it is mostly not used.</p>

<h3>Large array and better cache utilization</h3>

<p>Large Arrays in Hot Spot JVM are placed in contiguous memory as many other memory allocators try to do. However, all their elements might not fit into cache. An alternative is to split the array up into smaller ones so that each one fits into CPU-cache. The size of such a small array depends on many factors. I&rsquo;ll try to explain how to tune array-sizes in later posts.</p>

<h2>**Hash tables</h2>

<p>**</p>

<p>Hash tables exhibit poor locality of reference, because they cause access patterns that jump around, this can trigger <a href="http://en.wikipedia.org/wiki/CPU_cache">microprocessor cache</a> misses that cause long delays. Randomization causes bad locality. Compact data structures such as arrays searched with <a href="http://en.wikipedia.org/wiki/Linear_search">linear search</a> may be faster in some cases. An interesting article <a href="http://www.siam.org/meetings/alenex05/papers/13gheileman.pdf">&ldquo;How caching affects hashing&rdquo;</a> reveals the fact that the number of collisions a hash table produces, may lead to superior performance. In this paper three basic approaches were analyzed: linear probing, linear double hashing and exponential double hashing. All three make up so called <strong>Open addressing</strong> or <strong>closed hashing</strong> method of collision resolution. Open addressing can lead to more concise tables yielding a better cache performance than classing bucketing. But as the load factor starts to get high its performance might downgrade. The experimental analysis provided that, assuming a nearly uniform distribution, linear probing outperforms double hashing due to the fact that the percentage of the cache hits per probe is higher in the case of linear probing provided that its data set is not very large. However, If data doesn&rsquo;t fit into memory, linear probing may work slower. A great minus of this type of implementation is that the operation of deletion in open addressing is very expensive because of its O(n) worst time complexity on the array.</p>

<p>If a hash table tends to have many collisions, we can apply &ldquo;Unrolled linked list&rdquo; described above. Ideally each linked list&rsquo;s element should occupy one cache line on the appropriate cache level.  A great minus is that the size of a cache line is CPU-architecture-bound.</p>

<h2><strong>Binary Search Trees</strong></h2>

<p>Here I consider classic unbalanced Binary Searh trees (aka BST), Red-Black trees, AVL-trees and Splay-trees (aka The splay tree of Sleator and Tarjan) in terms of locality. Each tree should be applied in a different situation. All of these are linked data structures made up of nodes. Each node have 3 pointers: parent, left child and right child. Locality in trees is a tendency to look for the same element multiple times. Note that a set of operations exhibits no locality if every element is equally likely to be accessed at each point. Therefore, we&rsquo;re going to consider here only those cases where elements are accessed multiple times.</p>

<h3>Splay-trees</h3>

<p><a href="http://en.wikipedia.org/wiki/Splay_tree">Splay-trees</a> are the most intriguing due to the fact that they simply have the ability to optimize themselves for better locality of reference. The tree performs rotations of nodes to the root every time an access is made. Also note that they are not balanced trees as BST. Despite the fact that a worst case bound on the Splay-operation is O(n) for n nodes the amortized time for a set of operations is quite efficient (O(lg n)) which is compensated by these rotations and locality. Here we are talking about &ldquo;top-down splay-tree&rdquo; variation. Splay trees are the winner of locality among these ones when insertions happen quite frequently in sorted order and later accesses are sequential. Frequently used nodes are located near the root. In almost all other cases, because of the high cost of maintaining self-adjustment. Random insertion is the worst among all 4 data structures due to splay-overhear. On the contrary, the AVL-tree outperforms a Splay-trees when the search-key distribution is uniform and very frequent.</p>

<h3>AVL-trees</h3>

<p>If insertions happen quite frequently in sorted order the locality of <a href="http://en.wikipedia.org/wiki/AVL_tree">AVL-trees</a> is quite good provided that later accesses are more random. In other cases it may carry out far more comparisons than other trees which deteriorates its performance and therefore may stand behind others. The search performance of the AVL-tree is almost always better than that of Splay-trees.</p>

<h3>Red-Black trees</h3>

<p>If input is randomly ordered but sequential traversal happen frequently then <a href="http://en.wikipedia.org/wiki/Red%E2%80%93black_tree">red-black trees</a> should be used. Random insertions perform better over AVL-trees. However, for pathological inputs AVL-operations work faster than in Red-Black tree due to the stricter rule of rebalancing.</p>

<h3>Unbalanced BSTs</h3>

<p>When randomly ordered input can be relied upon it is best to use this basic kind of <a href="http://en.wikipedia.org/wiki/Binary_search_tree">binary search trees</a> over others. It requires the least extra overhead unlike the other tree-structures.</p>

<p><strong>The bottom line:  </strong>Input set and distribution of data both matter. In addition to locality, sometimes other factors are much more important for performance.</p>

<p>For random input set: BST - is the fastest among 4 remaining ones, then goes Red-Black tree, then goes the AVL-tree and the slowest one is a Splay-tree.</p>

<p>Splay trees take much of the CPU-time mostly on rotations where they lose in speed. There are some optimizations towards fewer splay-operations for certain cases, but they are not discussed in this blog. Unbalanced BSTs are simpler in implementation and have lighter operations and only best work against random data.</p>

<p>For pathological input set the picture is the opposite - from fastest to slowest: Splay-tree due to high locality, AVL-tree, Red-Black tree, BST - is extremely slow as it is unbalanced.</p>

<p>As this series is devoted solely to locality and some facts mentioned are not directly related to it, in later series I&rsquo;ll try to give some empirical benchmarks on overall performance of these structures to make the picture more clear.</p>

<h2><strong>Conclusion</strong></h2>

<p>it is worth noting that <em>hidden constants</em> caused by locality of reference might differ depending on computer architecture and implementation. Multiple operations on data structures with non-sequential access to elements cause poor performance. Asymptotic comparison of cache-friendly data structures with others is meaningless because in reality the result can be quite the contrary. Defragmented location of related elements in memory causes CPU cache-losses which can drastically degrade overall performance. Especially It is sensible on large data volumes where low latency is at premium. Mind you, algorithm with good locality is not sufficient for better performance. A number of operations, their cost including CPU-time do matter too.</p>
</div>
  
  


    </article>
  
  
    <article>
      
  <header>
    
      <h1 class="entry-title"><a href="/2013/01/11/when-asymptotic-analysis-of-complexity-doesnt-work/">Asymptotic Complexity: Beware of Hidden Constants</a></h1>
    
    
      <p class="meta">
        




<time class='entry-date' datetime='2013-01-11T15:24:57+04:00'><span class='date'><span class='date-month'>Jan</span> <span class='date-day'>11</span><span class='date-suffix'>th</span>, <span class='date-year'>2013</span></span> <span class='time'>3:24 pm</span></time>
        
           | <a href="/2013/01/11/when-asymptotic-analysis-of-complexity-doesnt-work/#disqus_thread"
             data-disqus-identifier="http://vibneiro.github.io/2013/01/11/when-asymptotic-analysis-of-complexity-doesnt-work/">Comments</a>
        
      </p>
    
  </header>


  <div class="entry-content"><h3><strong>Asymptotic complexity and invisible constant factor</strong></h3>

<p>Today I&rsquo;m going to explain what stands behind asymptotic complexity of algorithms when it comes to measurement of performance on modern computer hardware. Let&rsquo;s recall that asymptotic analysis is based on idealized sequential RAM-model. Complexity shows how good an algorithm scales as N grows from mathematical point of view. Two or more algorithms with different complexity solving the same task DOES NOT mean one will work slower or faster. Computer architecture is more complex than <a href="http://en.wikipedia.org/wiki/Random-access_machine">RAM-model</a>. There are different memory hierarchies, branching strategies, prefetching techniques, pipelining and other hardware factors which are not taken into account in this model. Constant-factors hidden in Big-Oh notation are not fixed, but variable in fact. Popular books on algorithms not always explain clearly this fact. While Big-Oh has many theoretically useful insights, it cannot actually predict performance. Read on, to find out why.</p>

<p><strong>1. Constant hidden even in simple arithmetic operations is not a constant at all:</strong></p>

<p>In terms of sequential Random Access Model and computer hardware we now try to add two numbers without a calculator using our brain: 1+2, 2+10, 34+55. A piece of cake! OK, so far so good. Try again, now: 34523+119987. Feel the difference? It takes you much longer to calculate the latter. This analogy shows the difference in complexity. In the RAM model adding 2 numbers <strong>is always a constant-time</strong>, but in real computer hardware it&rsquo;s not like that as constant-factor is variable. On top of this, some instructions are slower than others. Take addition and division as an example. They are both O(1) but with different constant factors.</p>

<p><strong>2. Some algorithms require more instructions than others</strong></p>

<p>For instance, certain algorithms may make fewer instructions per one pass (e.g. copy, exchange, shift, whatever). This also partly makes up a hidden constant. Due to this fact they tend to work faster in some cases.</p>

<p><strong>3. Computer memory hierarchy gets in the way of analysis</strong></p>

<p>Computer memory in the analysis of complexity is treated as a linear array with uniform access times. In the analysis A superior algorithm is the one that executes fewer instructions. In reality the assumption that every memory access has equal cost is not valid.</p>

<h2><strong>Conclusion</strong></h2>

<p>I didn&rsquo;t mention compilers that make optimizations. All of these factors form a hidden constant in the big-Oh which spoils the analysis. Asymptotic analysis is built upon mathematical model which is machine-independent and thus fragile. Hidden constants might impact algorithm&rsquo;s scalability very heavily. Due to this fact performance benchmarks may yield the opposite results. On some data an algorithm might be much slower then one with better complexity. The most vicious enemy is memory-hierarchy in modern CPUs. Yes, Big-Oh still matters, but you should handle it with care. In the next post I&rsquo;ll try to show this on real examples.</p>
</div>
  
  


    </article>
  
  
    <article>
      
  <header>
    
      <h1 class="entry-title"><a href="/2012/12/13/brewers-cap-theorem-explained-base-versus-acid/">Brewer&#8217;s CAP Theorem Explained: BASE Versus ACID</a></h1>
    
    
      <p class="meta">
        




<time class='entry-date' datetime='2012-12-13T17:21:22+04:00'><span class='date'><span class='date-month'>Dec</span> <span class='date-day'>13</span><span class='date-suffix'>th</span>, <span class='date-year'>2012</span></span> <span class='time'>5:21 pm</span></time>
        
           | <a href="/2012/12/13/brewers-cap-theorem-explained-base-versus-acid/#disqus_thread"
             data-disqus-identifier="http://vibneiro.github.io/2012/12/13/brewers-cap-theorem-explained-base-versus-acid/">Comments</a>
        
      </p>
    
  </header>


  <div class="entry-content"><p>The goal of this article is to give more clarity to the theorem and show pros and cons of ACID and BASE models that might stand in the way of implementing distributed systems.</p>

<h2>What is CAP about?</h2>

<p>The<em> (CAP) theorem</em> (<strong>C</strong>onsistency, <strong>A</strong>vailability and <strong>P</strong>artitioning tolerance) was given by Eric Brewer, a professor at the University of California, Berkeley and one of the founders of Google, in 2001 in the keynote of Principles of Distributed Computing.</p>

<p>The theorem states:</p>

<blockquote>_**Though its desirable to have Consistency, High-Availability and Partition-tolerance in every system, unfortunately no system can achieve all three at the same time**._</blockquote>


<p>In other words a system can have at most two of three desirable properties at the same time <strong>in presence of errors</strong>.</p>

<p>Let&rsquo;s first give definitions to these 3 terms:</p>

<p><strong>Consistency: </strong>A service that is <em>consistent</em> should follow the rule of ordering for updates that spread across all replicas in a cluster - &ldquo;what you write is what you read&rdquo;, regardless of location.<em> </em>For example,  Client A writes 1 then 2 to location X, Client B cannot read 2 followed by 1.  This rule has another name <strong>&ldquo;Strong consistency&rdquo;.</strong><strong>
</strong></p>

<p><strong>Availability: </strong>A service should be available. There should be a guarantee that every request receives a response about whether it was successful or failed. If the system is not available it can be still consistent. However, <em>consistency</em> and <em>availability </em>cannot be achieved at the same time. This means that one has two choices on what to leave. <em>Relaxing consistency</em> will allow the system to remain highly available under the partitioning conditions (see next definition) and <em>strong consistency</em> means that under certain conditions the system will not be available.</p>

<p><strong>Partition tolerance: </strong>The system continues to operate despite arbitrary message loss or failure of part of the system. A simple example, when we have a cluster of N replicated nodes and for some reason a network is unavailable among some number of  nodes (e.g. a network cable got chopped). This leads to inability to synchronize data. Thus, only some part of the system doesn&rsquo;t work, the other one does. If you have a partition in your network, you lose either <em>consistency</em> (because you allow updates to both sides of the partition) or you lose <em>availability</em> (because you detect the error and shut down the system until the error condition is resolved).</p>

<p>There are lots of articles about this theorem these days around but not many of them reveal real meaning behind this, neither<strong> </strong><em>CAP theorem</em> talks about the normal operation of a distributed system when there are no errors. A simple meaning of this theorem is <strong>&ldquo;It is impossible for a protocol to guarantee both consistency and availability in a partition prone distributed system&rdquo;. </strong>This was mentioned above in examples.</p>

<p>Most of the NoSQL database system architectures favour one factor over the other:</p>

<ul>
<li><p><a href="http://en.wikipedia.org/wiki/BigTable">BigTable</a>, used by Google App engine, and <a href="http://hadoop.apache.org/hbase/">HBase</a>, which runs over Hadoop, claim to be strongly <em>consistent</em> within a data-center and <em>highly available</em> meaning there&rsquo;s an eventual consistency between data-centers. Updates are propagated to all replicas <em>asynchronously</em>.</p></li>
<li><p><a href="http://research.google.com/archive/spanner.html">Google Spanner</a>, a new globally-distributed, and synchronously-replicated database - the successor to BigTable. Updates are propagated to all replicas <em>synchronously</em>. Google Spanner supports<em> strong consistenc_y even in the the presence of wide-area replication unlike BigTable which can only support </em>eventual-consistent_ (see below for definitions) replication across data-centers.</p></li>
<li><p><a href="http://www.allthingsdistributed.com/2007/10/amazons_dynamo.html">Amazon’s Dynamo</a>, <a href="http://www.royans.net/arch/category/cassandra/">Cassandra</a> and<a href="http://wiki.basho.com/"> Riak</a> instead sacrifice <em>consistency</em> in favor of availability and partition tolerance. They achieve a weaker form of consistency known as <em>eventual consistency</em> – updates are propagated to all replicas asynchronously, without guarantees on the order of updates across replicas and when they will be applied.</p></li>
<li><p><a href="http://www.oracle.com/technetwork/products/nosqldb/overview/index.html">Oracle NoSQL</a> allows to choose a consistency policy which might affect performance depending on a level selected.</p></li>
<li><p><a href="http://cassandra.apache.org/">Apache Cassandra</a> is similar to BigTable, but it has a <em>tunable consistency model</em>.</p></li>
</ul>


<h2>ACID property</h2>

<p>Let&rsquo;s recall in brief what<em> <a href="http://en.wikipedia.org/wiki/ACID">ACID</a></em> (<strong>A</strong>tomicity, <strong>C</strong>onsistency, <strong>I</strong>solation and <strong>D</strong>urability) means in  traditional RDBMS community before moving to the next topic.</p>

<p>ACID transactions provide 4 properties which must be guaranteed:</p>

<p><strong>Atomicity:</strong> All of the operations in the transaction will complete, or none will. If one part of the transaction fails, the entire transaction fails.</p>

<p><strong>Consistency:</strong> The database will be in a consistent state when the transaction begins and ends. This property ensures that any transaction will bring the database from one valid state to another. In high availability environment this rule must be satisfied for all nodes in a cluster.</p>

<p><strong>Isolation:</strong> The transaction will behave as if it is the only operation being performed upon the database. Each transaction has to execute in total isolation from the rest.</p>

<p><strong>Durability:</strong> Upon completion of the transaction, the operation will not be reversed.</p>

<p>ACID is guaranteed by <a href="http://en.wikipedia.org/wiki/Two-phase_commit_protocol"><em>A Two-phase</em> commit</a> - a distributed algorithm that ensures this across multiple database instances when performing transaction.</p>

<h2>Eventual consistency (BASE) != Strong consistency</h2>

<p><em>Eventual consistency </em>(normally asynchronous transactions) is a form of a weaker consistency which allows to improve speed and availability, because <em>ACID</em> provides <em>strong consistency </em>(synchronous transactions) for partitioned databases and thus gets in the way of availability. A transaction that involves N nodes in a cluster that uses <em>2-phase commit</em> also reduces the<em> availability.</em> The term e_ventual consistency or as it is called <strong>BASE </strong>(<strong>B</strong>asically <strong>A</strong>vailable, <strong>S</strong>oft state, <strong>E</strong>ventual consistency)<em> is the opposite of <strong>ACID</strong> (<strong>A</strong>tomicity, <strong>C</strong>onsistency, <strong>I</strong>solation and <strong>D</strong>urability). Where <strong>ACID</strong> is pessimistic and requires consistency at the end of every operation, <strong>BASE</strong> is optimistic and accepts that the database consistency will be in a state of flux. </em>The e_ventual consistency <em><em>is simply an acknowledgement that there is an unbounded delay in propagating a change made on one machine to all the other copies which might lead to stale data. For instance, a distributed system maintains copies of shared data on multiple machines in a cluster to ensure high availability. When data gets updated in a cluster there might be some interval of time during which some of the copies will be updated, but others won&rsquo;t. </em>Eventually</em> the changes will be propagated to all remaining machines. That&rsquo;s why it is named <em>eventual consistency</em>.  <em>BASE</em> trades consistency for availability and doesn&rsquo;t give any ordering guarantees at all.  Eventual consistency has nothing to do with a single node systems since there’s no need for propagation. If the database system only supports eventual consistency, then the application will need to handle the possibility of reading stale (inconsistent) data. There are different techniques how it can be achieved as well as other forms of weak consistency and out of scope of this article.  Eventual consistency is only one form from the list of <a href="http://en.wikipedia.org/wiki/Consistency_model">consistency-models</a> which are out of scope of this article.</p>

<h2>NRW notation (Read-Your-Writes)</h2>

<p><em>NRW (Node, Read, Write)</em>  allows to analyse and tune how a distributed database will trade off consistency, read / write performance.</p>

<ul>
<li><p>N = the number of nodes that keep copies of a record distributed to.</p></li>
<li><p>W = the number of nodes that must successfully acknowledge a write to be successfully committed.</p></li>
<li><p>R = the number of nodes that must send back the same value of a unit of data for it to be accepted as read by the system.</p></li>
</ul>


<p>The majority of NoSQL databases use N>W>1 - more than one write must complete, but not all nodes need to be updated immediately.</p>

<p>When:</p>

<ul>
<li><p>W &lt; N - high write availability</p></li>
<li><p>R &lt; N - high read availability</p></li>
<li><p>W+R > N - is a strong consistency, read/write are fully overlapped</p></li>
<li><p>W+R &lt;= N - is an eventual consistency, meaning that there is no overlap in the read and write set;</p></li>
</ul>


<p>You can  set these parameters and see what you will get <a href="http://pbs.cs.berkeley.edu/#demo">online</a>.Thus, varying the parameters we can tune a wide variety of scenarios with different properties of availability, consistency, reliability, and speed.</p>

<h2>Conclusion</h2>

<p>A <em>strongly consistent</em> system gives up <em>availability</em> upon a certain kind of failure, and<em> eventually-consistent</em> system gives up consistency upon a certain kind of failure which improves <em>availability</em>. The bottom line: It is impossible to guarantee consistency while providing high availability and network partition tolerance. This makes ACID databases less powerful for highly distributed environments and led to the emergence of alternate data stores that are target to high availability and high performance. The eventual consistency is one of approaches to achieve this.</p>
</div>
  
  


    </article>
  
  <div class="pagination">
    
    <a href="/blog/archives">Blog Archives</a>
    
    <a class="next" href="/posts/4">Newer &rarr;</a>
    
  </div>
</div>
<aside class="sidebar">
  
    <section>
  <h1>Welcome to my blog</h1>
  <script src="//about.me/embed/ivan_voroshilin?headline=0"></script>

</section>
<section>
  <h1>Recent Posts</h1>
  <ul id="recent_posts">
    
      <li class="post">
        <a href="/2015/06/08/researching-work-execution-with-dispatchers-in-java-8-from-naive-to-akka-like-design/">Concurrent Work Scheduling in Java 8: From Naive to Akka-like Dispatching</a>
      </li>
    
      <li class="post">
        <a href="/2015/03/13/migrated-to-octopress/">The Blog Has Migrated to a New Platform</a>
      </li>
    
      <li class="post">
        <a href="/2015/02/05/toughest-backtracking-problems-in-algorithmic-competitions/">Toughest Backtracking Problems in Algorithmic Competitions</a>
      </li>
    
      <li class="post">
        <a href="/2014/12/16/docker-creating-and-testing-httprest-server-on-top-of-akkaspray/">Dockerizing Spray HTTP Server</a>
      </li>
    
      <li class="post">
        <a href="/2014/10/30/docker-a-birds-eye-view/">Docker: A Bird&#8217;s-eye View</a>
      </li>
    
      <li class="post">
        <a href="/2014/10/12/the-flip-side-of-rule-engines-and-some-tips-on-when-not-use-ones/">The Flip Side of Rule Engines on Example of Drools and Some Valuable Tips</a>
      </li>
    
      <li class="post">
        <a href="/2014/09/15/project-euler-a-list-of-interesting-problems/">Project Euler: A List of Interesting Problems</a>
      </li>
    
      <li class="post">
        <a href="/2014/08/14/command-and-query-responsibility-segregation-and-event-sourcing-what-you-should-think-about-in-advance/">Command and Query Responsibility Segregation and Event Sourcing: What You Should Think About in Advance</a>
      </li>
    
      <li class="post">
        <a href="/2014/03/18/distributed-transactions-and-scalability-issues-in-large-scale-distributed-systems/">Distributed Transactions and Scalability Issues in Large-scale Distributed Systems</a>
      </li>
    
      <li class="post">
        <a href="/2014/02/17/service-discovery-in-distributed-systems/">Service Discovery in Distributed Systems</a>
      </li>
    
  </ul>
</section>

<section>
  <h1>GitHub Repos</h1>
  <ul id="gh_repos">
    <li class="loading">Status updating&#8230;</li>
  </ul>
  
  <script type="text/javascript">
    $(document).ready(function(){
        if (!window.jXHR){
            var jxhr = document.createElement('script');
            jxhr.type = 'text/javascript';
            jxhr.src = '/javascripts/libs/jXHR.js';
            var s = document.getElementsByTagName('script')[0];
            s.parentNode.insertBefore(jxhr, s);
        }

        github.showRepos({
            user: 'vibneiro',
            count: 10,
            skip_forks: true,
            target: '#gh_repos'
        });
    });
  </script>
  <script src="/javascripts/github.js" type="text/javascript"> </script>
</section>





  
</aside>

    </div>
  </div>
  <footer role="contentinfo"><p>
  Copyright &copy; 2015 - Ivan Voroshilin
</p>


<script>
  (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
  (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
  m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
  })(window,document,'script','//www.google-analytics.com/analytics.js','ga');

  ga('create', 'UA-37693662-1', 'ivoroshilin.com');
  ga('send', 'pageview');

</script>


</footer>
  

<script type="text/javascript">
      var disqus_shortname = 'vibneiro';
      
        
        var disqus_script = 'count.js';
      
    (function () {
      var dsq = document.createElement('script'); dsq.type = 'text/javascript'; dsq.async = true;
      dsq.src = '//' + disqus_shortname + '.disqus.com/' + disqus_script;
      (document.getElementsByTagName('head')[0] || document.getElementsByTagName('body')[0]).appendChild(dsq);
    }());
</script>









<script>
  (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
  (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
  m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
  })(window,document,'script','//www.google-analytics.com/analytics.js','ga');

  ga('create', 'UA-37693662-1', 'ivoroshilin.com');
  ga('send', 'pageview');

</script>






</body>
</html>
